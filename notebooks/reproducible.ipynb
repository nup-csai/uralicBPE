{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: ../tokenizers/uralic.txt: open: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wc -l ../tokenizers/uralic.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# @created: 20.09.2024\n",
    "# @author: Aleksey Komissarov\n",
    "# @contact: ad3002@gmail.com\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_bytes_char_mapping():\n",
    "    \"\"\"\n",
    "    Generates byte-to-character and character-to-byte mappings consistent with the Rust encoder.\n",
    "\n",
    "    Returns:\n",
    "        byte_to_char (dict): Maps each byte (0-255) to a unique Unicode character.\n",
    "        char_to_byte (dict): Reverse mapping from Unicode characters to bytes.\n",
    "    \"\"\"\n",
    "    # Step 1: Define the initial bytes (188 bytes)\n",
    "    initial_bytes = list(range(0x21, 0x7F))  # 0x21 (!) to 0x7E (~)\n",
    "    initial_bytes += list(range(0xA1, 0xAD))  # 0xA1 to 0xAC\n",
    "    initial_bytes += list(range(0xAE, 0x100))  # 0xAE to 0xFF\n",
    "\n",
    "    # Step 2: Identify missing bytes (68 bytes)\n",
    "    all_bytes = set(range(256))\n",
    "    present_bytes = set(initial_bytes)\n",
    "    missing_bytes = sorted(all_bytes - present_bytes)\n",
    "\n",
    "    # Step 3: Create mappings\n",
    "    byte_to_char = {}\n",
    "    char_to_byte = {}\n",
    "\n",
    "    # Map initial bytes to their direct Unicode equivalents\n",
    "    for byte in initial_bytes:\n",
    "        char = chr(byte)\n",
    "        byte_to_char[byte] = char\n",
    "        char_to_byte[char] = byte\n",
    "\n",
    "    # Map missing bytes to unique Unicode characters starting from U+0100\n",
    "    start_code_point = 0x0100  # U+0100 (Ā)\n",
    "    for i, byte in enumerate(missing_bytes):\n",
    "        char = chr(start_code_point + i)\n",
    "        byte_to_char[byte] = char\n",
    "        char_to_byte[char] = byte\n",
    "\n",
    "    return byte_to_char, char_to_byte\n",
    "\n",
    "# Generate the mappings\n",
    "byte_to_char, char_to_byte = generate_bytes_char_mapping()\n",
    "\n",
    "\n",
    "def byte_level_decode(encoded_string, char_to_byte, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Decodes a ByteLevel encoded string back to the original string using the provided mapping.\n",
    "\n",
    "    Parameters:\n",
    "        encoded_string (str): The ByteLevel encoded string.\n",
    "        char_to_byte (dict): Mapping from Unicode characters to byte values.\n",
    "        encoding (str): The encoding to use for the output string (default: 'utf-8').\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded original string.\n",
    "    \"\"\"\n",
    "    decoded_bytes = bytearray()\n",
    "    for char in encoded_string:\n",
    "        if char in char_to_byte:\n",
    "            decoded_bytes.append(char_to_byte[char])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown character in encoded string: {char}\")\n",
    "            # Alternatively, use a placeholder:\n",
    "            # decoded_bytes.append(ord('?'))\n",
    "\n",
    "    try:\n",
    "        return decoded_bytes.decode(encoding)\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise UnicodeDecodeError(f\"Failed to decode byte sequence: {e}\")\n",
    "\n",
    "def byte_level_decode_custom(encoded_string, char_to_byte, encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    Decodes a ByteLevel encoded string back to the original string using the provided mapping.\n",
    "    Handles invalid UTF-8 sequences by replacing them with � and incomplete bytes at the end with <0xXX>.\n",
    "\n",
    "    Parameters:\n",
    "        encoded_string (str): The ByteLevel encoded string.\n",
    "        char_to_byte (dict): Mapping from Unicode characters to byte values.\n",
    "        encoding (str): The encoding to use for the output string (default: 'utf-8').\n",
    "\n",
    "    Returns:\n",
    "        str: The decoded original string with � for invalid sequences and <0xXX> for incomplete bytes.\n",
    "    \"\"\"\n",
    "    # Step 1: Convert encoded characters back to bytes\n",
    "    byte_seq = bytearray()\n",
    "    for char in encoded_string:\n",
    "        if char in char_to_byte:\n",
    "            byte_seq.append(char_to_byte[char])\n",
    "        else:\n",
    "            # Handle unknown characters by replacing with � (0xFF)\n",
    "            byte_seq.append(0xFF)  # You can choose a different placeholder if needed\n",
    "\n",
    "    # Step 2: Iterate through the byte sequence to decode UTF-8 characters\n",
    "    decoded_chars = []\n",
    "    i = 0\n",
    "    n = len(byte_seq)\n",
    "\n",
    "    while i < n:\n",
    "        byte = byte_seq[i]\n",
    "        # Single-byte (ASCII)\n",
    "        if byte <= 0x7F:\n",
    "            decoded_chars.append(chr(byte))\n",
    "            i += 1\n",
    "        # Two-byte sequence\n",
    "        elif 0xC0 <= byte <= 0xDF:\n",
    "            if i + 1 < n:\n",
    "                next_byte = byte_seq[i + 1]\n",
    "                if 0x80 <= next_byte <= 0xBF:\n",
    "                    try:\n",
    "                        char = bytes(byte_seq[i:i+2]).decode(encoding)\n",
    "                        decoded_chars.append(char)\n",
    "                        i += 2\n",
    "                        continue\n",
    "                    except UnicodeDecodeError:\n",
    "                        pass\n",
    "            # Invalid continuation byte\n",
    "            decoded_chars.append('�')\n",
    "            i += 1\n",
    "        # Three-byte sequence\n",
    "        elif 0xE0 <= byte <= 0xEF:\n",
    "            if i + 2 < n:\n",
    "                next1 = byte_seq[i + 1]\n",
    "                next2 = byte_seq[i + 2]\n",
    "                if 0x80 <= next1 <= 0xBF and 0x80 <= next2 <= 0xBF:\n",
    "                    try:\n",
    "                        char = bytes(byte_seq[i:i+3]).decode(encoding)\n",
    "                        decoded_chars.append(char)\n",
    "                        i += 3\n",
    "                        continue\n",
    "                    except UnicodeDecodeError:\n",
    "                        pass\n",
    "            # Invalid continuation bytes\n",
    "            decoded_chars.append('�')\n",
    "            i += 1\n",
    "        # Four-byte sequence\n",
    "        elif 0xF0 <= byte <= 0xF7:\n",
    "            if i + 3 < n:\n",
    "                next1 = byte_seq[i + 1]\n",
    "                next2 = byte_seq[i + 2]\n",
    "                next3 = byte_seq[i + 3]\n",
    "                if 0x80 <= next1 <= 0xBF and 0x80 <= next2 <= 0xBF and 0x80 <= next3 <= 0xBF:\n",
    "                    try:\n",
    "                        char = bytes(byte_seq[i:i+4]).decode(encoding)\n",
    "                        decoded_chars.append(char)\n",
    "                        i += 4\n",
    "                        continue\n",
    "                    except UnicodeDecodeError:\n",
    "                        pass\n",
    "            # Invalid continuation bytes\n",
    "            decoded_chars.append('�')\n",
    "            i += 1\n",
    "        else:\n",
    "            # Invalid start byte\n",
    "            decoded_chars.append('�')\n",
    "            i += 1\n",
    "\n",
    "    # Step 3: Check for incomplete bytes at the end\n",
    "    # In this implementation, incomplete bytes are already handled by replacing with �\n",
    "    # If you want to represent incomplete bytes specifically, additional logic is needed\n",
    "\n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "\n",
    "\n",
    "def load_vocab(tokenizer_file):\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    try:\n",
    "        with open(tokenizer_file, \"r\") as fr:\n",
    "            tokenizer = json.load(fr)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print(f\"Bad tokenizer file: {tokenizer_file}\")\n",
    "        print(\"Please provide a valid tokenizer file.\")\n",
    "        print(\"Here are the first few lines of the file:\")\n",
    "        with open(tokenizer_file, \"r\") as fr:\n",
    "            for i, line in enumerate(fr):\n",
    "                print(line)\n",
    "                if i > 10:\n",
    "                    break\n",
    "        sys.exit(1)\n",
    "    if not \"model\" in tokenizer:\n",
    "        if \"vocab\" in tokenizer:\n",
    "            tokenizer[\"model\"] = {\"vocab\": tokenizer[\"vocab\"]}\n",
    "        if \"mama\" in tokenizer:\n",
    "            tokenizer[\"model\"] = {\n",
    "                \"vocab\": tokenizer\n",
    "            }\n",
    "        with open(tokenizer_file, \"w\") as fw:\n",
    "            json.dump(tokenizer, fw, indent=2)\n",
    "\n",
    "    ### rare case with negative ranks\n",
    "    if \"model\" in tokenizer and \"vocab\" in tokenizer[\"model\"]:\n",
    "        if isinstance(tokenizer[\"model\"][\"vocab\"], list):\n",
    "            print(\"Bad format for vocab\")\n",
    "            sys.exit(1)\n",
    "    if not \"model\" in tokenizer or not \"vocab\" in tokenizer[\"model\"]:\n",
    "        print(\"Bad format for vocab\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    with open(tokenizer_file) as fh:\n",
    "        text_data = fh.read()\n",
    "\n",
    "    replacers = [\n",
    "         ( text_data.count(\"▁\"), \"▁\"),\n",
    "         ( text_data.count(\"Ġ\"), \"Ġ\"),\n",
    "         ( text_data.count(\"\\u0120\"), \"\\u0120\"),\n",
    "         ( text_data.count(\"\\t\"), \"\\t\"),\n",
    "         ( text_data.count(\"\\u2581\"), \"\\u2581\"),\n",
    "    ]\n",
    "    replacers.sort()\n",
    "    replace = replacers[-1][1]\n",
    "    if replace == \"Ġ\":\n",
    "        replace = None\n",
    "       \n",
    "        \n",
    "    should_be_fixed = \"ма\" not in text_data\n",
    "    for raw_token, rid, in tokenizer[\"model\"][\"vocab\"].items():\n",
    "        \n",
    "        rr = raw_token\n",
    "        if replace and raw_token.startswith(replace) and len(raw_token) > 1:\n",
    "          if should_be_fixed:\n",
    "            raw_token = \"Ġ\" + replace.join(raw_token.split(replace)[1:])\n",
    "          else:\n",
    "            raw_token = \" \" + replace.join(raw_token.split(replace)[1:])\n",
    "            \n",
    "        if raw_token.lower().startswith(\"<0x\"):\n",
    "          token = byte_to_char[eval(raw_token[1:-1])]\n",
    "          vocab[token] = rid\n",
    "          continue\n",
    "    \n",
    "        if should_be_fixed:\n",
    "          token =  byte_level_decode_custom(raw_token, char_to_byte)\n",
    "          if [1 for x in token if ord(x) == 65533]:\n",
    "            token = f\"<0y{raw_token}>\"\n",
    "        else:\n",
    "          if raw_token in char_to_byte:\n",
    "            token = str(hex(char_to_byte[raw_token])).upper()\n",
    "          else:\n",
    "            token = raw_token\n",
    "        try:\n",
    "          assert token not in vocab\n",
    "        except:\n",
    "          print(f\"ERROR-{rid}-{token}-|-{raw_token}-{rr}-{len(raw_token)}\")\n",
    "          print([ord(x) for x in token])\n",
    "          input(\"?\")\n",
    "\n",
    "        vocab[token] = rid\n",
    "\n",
    "    if \"added_tokens\" in tokenizer:\n",
    "      for d in tokenizer[\"added_tokens\"]:\n",
    "        raw_token = d[\"content\"]\n",
    "        rid = d[\"id\"]\n",
    "        if not raw_token in vocab:\n",
    "          vocab[raw_token] = rid\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def load_vocab(tokenizer_file):\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    try:\n",
    "        with open(tokenizer_file, \"r\") as fr:\n",
    "            tokenizer = json.load(fr)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print(f\"Bad tokenizer file: {tokenizer_file}\")\n",
    "        print(\"Please provide a valid tokenizer file.\")\n",
    "        print(\"Here are the first few lines of the file:\")\n",
    "        with open(tokenizer_file, \"r\") as fr:\n",
    "            for i, line in enumerate(fr):\n",
    "                print(line)\n",
    "                if i > 10:\n",
    "                    break\n",
    "        sys.exit(1)\n",
    "    if not \"model\" in tokenizer:\n",
    "        if \"vocab\" in tokenizer:\n",
    "            tokenizer[\"model\"] = {\"vocab\": tokenizer[\"vocab\"]}\n",
    "        if \"mama\" in tokenizer:\n",
    "            tokenizer[\"model\"] = {\n",
    "                \"vocab\": tokenizer\n",
    "            }\n",
    "        with open(tokenizer_file, \"w\") as fw:\n",
    "            json.dump(tokenizer, fw, indent=2)\n",
    "\n",
    "    ### rare case with negative ranks\n",
    "    if \"model\" in tokenizer and \"vocab\" in tokenizer[\"model\"]:\n",
    "        if isinstance(tokenizer[\"model\"][\"vocab\"], list):\n",
    "            print(\"Bad format for vocab\")\n",
    "            sys.exit(1)\n",
    "    if not \"model\" in tokenizer or not \"vocab\" in tokenizer[\"model\"]:\n",
    "        print(\"Bad format for vocab\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    with open(tokenizer_file) as fh:\n",
    "        text_data = fh.read()\n",
    "\n",
    "    replacers = [\n",
    "         ( text_data.count(\"▁\"), \"▁\"),\n",
    "         ( text_data.count(\"Ġ\"), \"Ġ\"),\n",
    "         ( text_data.count(\"\\u0120\"), \"\\u0120\"),\n",
    "         ( text_data.count(\"\\t\"), \"\\t\"),\n",
    "         ( text_data.count(\"\\u2581\"), \"\\u2581\"),\n",
    "    ]\n",
    "    replacers.sort()\n",
    "    replace = replacers[-1][1]\n",
    "    if replace == \"Ġ\":\n",
    "        replace = None\n",
    "       \n",
    "        \n",
    "    should_be_fixed = \"ма\" not in text_data\n",
    "    for raw_token, rid, in tqdm(tokenizer[\"model\"][\"vocab\"].items()):\n",
    "        \n",
    "        rr = raw_token\n",
    "        if replace and raw_token.startswith(replace) and len(raw_token) > 1:\n",
    "          if should_be_fixed:\n",
    "            raw_token = \"Ġ\" + replace.join(raw_token.split(replace)[1:])\n",
    "          else:\n",
    "            raw_token = \" \" + replace.join(raw_token.split(replace)[1:])\n",
    "            \n",
    "        if raw_token.lower().startswith(\"<0x\"):\n",
    "          token = byte_to_char[eval(raw_token[1:-1])]\n",
    "          vocab[token] = rid\n",
    "          continue\n",
    "    \n",
    "        if should_be_fixed:\n",
    "          token =  byte_level_decode_custom(raw_token, char_to_byte)\n",
    "          if [1 for x in token if ord(x) == 65533]:\n",
    "            token = f\"<0y{raw_token}>\"\n",
    "        else:\n",
    "          if raw_token in char_to_byte:\n",
    "            token = str(hex(char_to_byte[raw_token])).upper()\n",
    "          else:\n",
    "            token = raw_token\n",
    "        try:\n",
    "          assert token not in vocab\n",
    "        except:\n",
    "          print(f\"ERROR-{rid}-{token}-|-{raw_token}-{rr}-{len(raw_token)}\")\n",
    "          print([ord(x) for x in token])\n",
    "          input(\"?\")\n",
    "\n",
    "        vocab[token] = rid\n",
    "\n",
    "    if \"added_tokens\" in tokenizer:\n",
    "      for d in tokenizer[\"added_tokens\"]:\n",
    "        raw_token = d[\"content\"]\n",
    "        rid = d[\"id\"]\n",
    "        if not raw_token in vocab:\n",
    "          vocab[raw_token] = rid\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizer import load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_files = {\n",
    "    \"estonian\": \"../tokenizers/et.wiki_paragraphs.2024-05.json\",\n",
    "    \"finnish\": \"../tokenizers/fi.wiki_paragraphs.2023-05.json\",\n",
    "    \"hungarian\": \"../tokenizers/hu.wiki_paragraphs.2024-05.json\",\n",
    "    \"se\": \"../tokenizers/se.wiki_paragraphs.2024-05.json\",\n",
    "    \"uralic\": \"../tokenizers/uralic.wiki_paragraphs.2024-05.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 256000/256000 [00:00<00:00, 287965.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 256000/256000 [00:00<00:00, 292348.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 256000/256000 [00:00<00:00, 274303.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████| 93188/93188 [00:00<00:00, 293944.66it/s]\n",
      "100%|█████████████████████████████████████████████████████████| 256000/256000 [00:00<00:00, 292109.71it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabs = {k: load_vocab(v) for k, v in tokenizer_files.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = set()\n",
    "for k, v in vocabs.items():\n",
    "    for token in v:\n",
    "        all_tokens.add(token)\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estonian 256000\n",
      "finnish 256000\n",
      "hungarian 256000\n",
      "se 93188\n",
      "uralic 256000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "785115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_ranks = {}\n",
    "for i, (k, v) in enumerate(vocabs.items()):\n",
    "    print(k, len(v))\n",
    "    for token in v:\n",
    "        token_to_ranks.setdefault(token, [0] * len(vocabs))\n",
    "        token_to_ranks[token][i] = v[token] + 1\n",
    "len(token_to_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [1, 1, 1, 1, 1]\n",
      "<pad> [2, 2, 2, 2, 2]\n",
      "</s> [3, 3, 3, 3, 3]\n",
      "<unk> [4, 4, 4, 4, 4]\n",
      "<mask> [5, 5, 5, 5, 5]\n",
      "A [6, 6, 6, 19, 6]\n",
      "B [7, 7, 7, 20, 7]\n",
      "C [8, 8, 8, 21, 8]\n",
      "D [9, 9, 9, 22, 9]\n",
      "E [10, 10, 10, 23, 10]\n",
      "F [11, 11, 11, 24, 11]\n",
      "G [12, 12, 12, 25, 12]\n",
      "H [13, 13, 13, 26, 13]\n",
      "I [14, 14, 14, 27, 14]\n",
      "J [15, 15, 15, 28, 15]\n",
      "K [16, 16, 16, 29, 16]\n",
      "L [17, 17, 17, 30, 17]\n",
      "M [18, 18, 18, 31, 18]\n",
      "N [19, 19, 19, 32, 19]\n",
      "O [20, 20, 20, 33, 20]\n",
      "P [21, 21, 21, 34, 21]\n",
      "Q [22, 22, 22, 35, 22]\n",
      "R [23, 23, 23, 36, 23]\n",
      "S [24, 24, 24, 37, 24]\n",
      "T [25, 25, 25, 38, 25]\n",
      "U [26, 26, 26, 39, 26]\n",
      "V [27, 27, 27, 40, 27]\n",
      "W [28, 28, 28, 41, 28]\n",
      "X [29, 29, 29, 42, 29]\n",
      "Y [30, 30, 30, 43, 30]\n",
      "Z [31, 31, 31, 44, 31]\n",
      "a [32, 32, 32, 45, 32]\n",
      "b [33, 33, 33, 46, 33]\n",
      "c [34, 34, 34, 47, 34]\n",
      "d [35, 35, 35, 48, 35]\n",
      "e [36, 36, 36, 49, 36]\n",
      "f [37, 37, 37, 50, 37]\n",
      "g [38, 38, 38, 51, 38]\n",
      "h [39, 39, 39, 52, 39]\n",
      "i [40, 40, 40, 53, 40]\n",
      "j [41, 41, 41, 54, 41]\n",
      "k [42, 42, 42, 55, 42]\n",
      "l [43, 43, 43, 56, 43]\n",
      "m [44, 44, 44, 57, 44]\n",
      "n [45, 45, 45, 58, 45]\n",
      "o [46, 46, 46, 59, 46]\n",
      "p [47, 47, 47, 60, 47]\n",
      "q [48, 48, 48, 61, 48]\n",
      "r [49, 49, 49, 62, 49]\n",
      "s [50, 50, 50, 63, 50]\n",
      "t [51, 51, 51, 64, 51]\n",
      "u [52, 52, 52, 65, 52]\n",
      "v [53, 53, 53, 66, 53]\n",
      "w [54, 54, 54, 67, 54]\n",
      "x [55, 55, 55, 68, 55]\n",
      "y [56, 56, 56, 69, 56]\n",
      "z [57, 57, 57, 70, 57]\n",
      "<0y¡> [58, 58, 58, 71, 58]\n",
      "<0y¢> [59, 59, 59, 72, 59]\n",
      "<0y£> [60, 60, 60, 73, 60]\n",
      "<0y¤> [61, 61, 61, 74, 61]\n",
      "<0y¥> [62, 62, 62, 75, 62]\n",
      "<0y¦> [63, 63, 63, 76, 63]\n",
      "<0y§> [64, 64, 64, 77, 64]\n",
      "<0y¨> [65, 65, 65, 78, 65]\n",
      "<0y©> [66, 66, 66, 79, 66]\n",
      "<0yª> [67, 67, 67, 80, 67]\n",
      "<0y«> [68, 68, 68, 81, 68]\n",
      "<0y¬> [69, 69, 69, 82, 69]\n",
      "<0y®> [70, 70, 70, 83, 70]\n",
      "<0y¯> [71, 71, 71, 84, 71]\n",
      "<0y°> [72, 72, 72, 85, 72]\n",
      "<0y±> [73, 73, 73, 86, 73]\n",
      "<0y²> [74, 74, 74, 87, 74]\n",
      "<0y³> [75, 75, 75, 88, 75]\n",
      "<0y´> [76, 76, 76, 89, 76]\n",
      "<0yµ> [77, 77, 77, 90, 77]\n",
      "<0y¶> [78, 78, 78, 91, 78]\n",
      "<0y·> [79, 79, 79, 92, 79]\n",
      "<0y¸> [80, 80, 80, 93, 80]\n",
      "<0y¹> [81, 81, 81, 94, 81]\n",
      "<0yº> [82, 82, 82, 95, 82]\n",
      "<0y»> [83, 83, 83, 96, 83]\n",
      "<0y¼> [84, 84, 84, 97, 84]\n",
      "<0y½> [85, 85, 85, 98, 85]\n",
      "<0y¾> [86, 86, 86, 99, 86]\n",
      "<0y¿> [87, 87, 87, 100, 87]\n",
      "<0yÃ> [88, 88, 88, 101, 88]\n",
      "<0yÄ> [89, 89, 89, 102, 89]\n",
      "<0yÅ> [90, 90, 90, 103, 90]\n",
      "<0yÆ> [91, 91, 91, 104, 91]\n",
      "<0yÇ> [92, 92, 92, 105, 92]\n",
      "<0yÈ> [93, 93, 93, 106, 93]\n",
      "<0yÉ> [94, 94, 94, 107, 94]\n",
      "<0yÊ> [95, 95, 95, 108, 95]\n",
      "<0yá> [96, 96, 96, 109, 96]\n",
      "<0yâ> [97, 97, 97, 0, 97]\n",
      "<0yê> [98, 98, 98, 0, 98]\n",
      "<0yï> [99, 99, 99, 0, 99]\n",
      "  [100, 100, 100, 111, 100]\n",
      "<0yĢ> [101, 101, 101, 112, 101]\n",
      "<0yģ> [102, 102, 102, 113, 102]\n",
      "<0yĤ> [103, 103, 103, 114, 103]\n",
      "<0yĥ> [104, 104, 104, 115, 104]\n",
      "<0yĦ> [105, 105, 105, 116, 105]\n",
      "<0yħ> [106, 106, 106, 117, 106]\n",
      "<0yĨ> [107, 107, 107, 118, 107]\n",
      "<0yĩ> [108, 108, 108, 119, 108]\n",
      "<0yĪ> [109, 109, 109, 0, 109]\n",
      "<0yī> [110, 110, 110, 120, 110]\n",
      "<0yĬ> [111, 111, 111, 121, 111]\n",
      "<0yĭ> [112, 112, 112, 122, 112]\n",
      "<0yĮ> [113, 113, 113, 123, 113]\n",
      "<0yį> [114, 114, 114, 124, 114]\n",
      "<0yİ> [115, 115, 115, 125, 115]\n",
      "<0yı> [116, 116, 116, 126, 116]\n",
      "<0yĲ> [117, 117, 117, 127, 117]\n",
      "<0yĳ> [118, 118, 118, 128, 118]\n",
      "<0yĴ> [119, 119, 119, 129, 119]\n",
      "<0yĵ> [120, 120, 120, 130, 120]\n",
      "<0yĶ> [121, 121, 121, 131, 121]\n",
      "<0yķ> [122, 122, 122, 132, 122]\n",
      "<0yĸ> [123, 123, 123, 133, 123]\n",
      "<0yĹ> [124, 124, 124, 134, 124]\n",
      "<0yĺ> [125, 125, 125, 135, 125]\n",
      "<0yĻ> [126, 126, 126, 136, 126]\n",
      "<0yļ> [127, 127, 127, 137, 127]\n",
      "<0yĽ> [128, 128, 128, 138, 128]\n",
      "<0yľ> [129, 129, 129, 139, 129]\n",
      "<0yĿ> [130, 130, 130, 0, 130]\n",
      "<0yŀ> [131, 131, 131, 140, 131]\n",
      "<0yŁ> [132, 132, 132, 141, 132]\n",
      "<0ył> [133, 133, 133, 142, 133]\n",
      "<0yŃ> [134, 134, 134, 143, 134]\n",
      "st [135, 3518, 481, 269, 344]\n",
      "se [136, 316, 2621, 2820, 321]\n",
      " k [137, 141, 142, 261, 142]\n",
      "al [138, 150, 153, 194, 152]\n",
      "ä [139, 135, 3315, 200, 137]\n",
      "li [140, 154, 2307, 691, 468]\n",
      "õ [141, 7146, 22439, 1946, 241]\n",
      "ja [142, 180, 243, 268, 209]\n",
      " t [143, 932, 152, 380, 157]\n",
      "ri [144, 190, 400, 375, 327]\n",
      "on [145, 145, 165, 182, 150]\n",
      "mi [146, 204, 7453, 521, 723]\n",
      "le [147, 234, 726, 592, 235]\n",
      "ma [148, 184, 1978, 187, 289]\n",
      " v [149, 148, 154, 177, 151]\n",
      "si [150, 146, 5112, 3265, 196]\n",
      " a [151, 200, 137, 239, 139]\n",
      " p [152, 151, 200, 263, 163]\n",
      "ni [153, 460, 285, 2626, 342]\n",
      "at [154, 671, 150, 170, 158]\n",
      "ü [155, 1737, 167, 1987, 181]\n",
      "ne [156, 271, 788, 636, 609]\n",
      " e [157, 191, 652, 451, 193]\n",
      "de [158, 239, 3456, 301, 958]\n",
      "oo [159, 655, 10600, 2668, 2972]\n",
      " s [160, 155, 205, 160, 148]\n",
      "ast [161, 4424, 1030, 502, 357]\n",
      "as [162, 158, 232, 159, 159]\n",
      "us [163, 174, 225, 176, 161]\n",
      " ja [164, 156, 19540, 163, 184]\n",
      "me [165, 501, 3975, 2587, 1080]\n",
      "te [166, 152, 310, 924, 234]\n",
      " o [167, 160, 7366, 169, 1110]\n",
      "ud [168, 474, 318, 456, 223]\n",
      "nd [169, 0, 30892, 1369, 14713]\n",
      "na [170, 222, 567, 228, 280]\n",
      "ks [171, 186, 26604, 427, 483]\n",
      "la [172, 243, 1620, 154, 199]\n",
      "ga [173, 952, 1977, 188, 816]\n",
      "ra [174, 197, 260, 231, 285]\n",
      "re [175, 321, 258, 421, 320]\n",
      "is [176, 138, 179, 167, 143]\n",
      " on [177, 179, 3101, 7077, 218]\n",
      "id [178, 848, 343, 174, 204]\n",
      "uu [179, 163, 92977, 1228, 746]\n",
      "use [180, 1298, 11242, 3726, 443]\n",
      "it [181, 233, 230, 179, 160]\n",
      "ad [182, 4064, 209, 395, 216]\n",
      "ik [183, 173, 182, 529, 162]\n",
      "ng [184, 14510, 12710, 3129, 11637]\n",
      "el [185, 167, 139, 252, 141]\n",
      " S [186, 178, 217, 178, 188]\n",
      "ust [187, 5688, 1641, 547, 353]\n",
      " m [188, 149, 145, 161, 147]\n",
      "ut [189, 231, 229, 276, 190]\n",
      " te [190, 263, 3029, 3440, 530]\n",
      " K [191, 194, 237, 214, 205]\n",
      " se [192, 247, 6403, 3821, 369]\n",
      " ka [193, 214, 12747, 5025, 298]\n",
      " j [194, 142, 212, 158, 155]\n",
      " T [195, 212, 257, 278, 221]\n",
      "ar [196, 171, 174, 184, 174]\n",
      "he [197, 280, 924, 1041, 557]\n",
      " aast [198, 0, 0, 0, 0]\n",
      " mi [199, 537, 557, 2318, 620]\n",
      " ko [200, 225, 4436, 4406, 581]\n",
      "es [201, 153, 168, 209, 153]\n",
      "da [202, 525, 2810, 153, 747]\n",
      "ö [203, 168, 143, 365, 154]\n",
      "ti [204, 159, 4235, 2163, 180]\n",
      "gi [205, 1164, 6549, 204, 778]\n",
      " ku [206, 310, 24821, 8136, 301]\n",
      " A [207, 213, 160, 245, 176]\n",
      "õi [208, 0, 0, 0, 26722]\n",
      " l [209, 169, 185, 147, 165]\n",
      " P [210, 215, 267, 251, 230]\n",
      "ku [211, 241, 42976, 485, 389]\n",
      "va [212, 175, 529, 500, 436]\n",
      "il [213, 181, 210, 236, 166]\n",
      "ta [214, 139, 268, 185, 140]\n",
      "ee [215, 567, 8923, 0, 1813]\n",
      " h [216, 170, 158, 232, 164]\n",
      "ge [217, 1100, 3135, 298, 1912]\n",
      "är [218, 667, 17357, 414, 309]\n",
      " sa [219, 219, 25227, 922, 459]\n",
      "in [220, 137, 171, 165, 144]\n",
      " M [221, 203, 216, 229, 207]\n",
      " ta [222, 206, 21664, 2753, 217]\n",
      " E [223, 251, 227, 296, 229]\n",
      " ü [224, 96314, 555, 13055, 529]\n",
      " L [225, 229, 271, 234, 239]\n",
      "lt [226, 164951, 806, 0, 1296]\n",
      "est [227, 1986, 350, 532, 271]\n",
      " ni [228, 511, 33278, 45195, 1034]\n",
      " V [229, 221, 316, 260, 246]\n",
      "mise [230, 24993, 235569, 0, 3059]\n",
      "ii [231, 5425, 20937, 151, 17773]\n",
      "or [232, 250, 157, 195, 168]\n",
      "er [233, 161, 144, 183, 149]\n",
      "an [234, 140, 149, 164, 145]\n",
      " oli [235, 217, 0, 0, 299]\n",
      "ak [236, 14355, 162, 635, 178]\n",
      "oon [237, 935, 12871, 0, 415]\n",
      " ma [238, 276, 1209, 378, 274]\n",
      " li [239, 351, 19851, 2025, 507]\n",
      " R [240, 249, 303, 227, 264]\n",
      "ist [241, 1311, 882, 357, 210]\n",
      "gu [242, 3098, 29202, 621, 5030]\n",
      " va [243, 267, 45795, 3551, 495]\n",
      " vä [244, 402, 235628, 44252, 1003]\n",
      " al [245, 232, 250, 345, 249]\n",
      "ro [246, 325, 961, 577, 812]\n",
      "ul [247, 210, 228, 306, 189]\n",
      " i [248, 1426, 265, 581, 823]\n",
      "ka [249, 164, 1169, 265, 263]\n",
      "tu [250, 157, 35858, 9316, 1566]\n",
      " või [251, 0, 0, 0, 1422]\n",
      "lle [252, 218, 0, 519, 32255]\n",
      " N [253, 269, 288, 237, 268]\n",
      " n [254, 182, 173, 196, 175]\n",
      " aastal [255, 0, 0, 0, 771]\n",
      "val [256, 426, 804, 648, 396]\n",
      " H [257, 199, 280, 246, 231]\n",
      "ki [258, 192, 954, 307, 326]\n",
      "di [259, 411, 4243, 338, 2111]\n",
      "nud [260, 0, 0, 0, 1594]\n",
      "vad [261, 56329, 4698, 5772, 1893]\n",
      "ab [262, 96189, 327, 701, 384]\n",
      "ol [263, 298, 161, 244, 156]\n",
      " val [264, 303, 376, 2487, 324]\n",
      "sa [265, 143, 2648, 354, 173]\n",
      "et [266, 185, 140, 207, 146]\n",
      " la [267, 311, 3636, 637, 549]\n",
      "lise [268, 212758, 0, 0, 100008]\n",
      "eri [269, 1047, 1562, 1397, 692]\n",
      "kon [270, 425, 13860, 814, 1859]\n",
      " ra [271, 306, 2203, 2096, 940]\n",
      "ub [272, 1249, 764, 1163, 587]\n",
      "ur [273, 257, 296, 273, 214]\n",
      "lik [274, 64863, 1574, 0, 786]\n",
      "öö [275, 3135, 235609, 49928, 2378]\n",
      " B [276, 258, 236, 257, 243]\n",
      " ning [277, 0, 172393, 0, 877]\n",
      " I [278, 302, 302, 332, 291]\n",
      " et [279, 1150, 2014, 1988, 579]\n",
      "sid [280, 69368, 105519, 0, 1313]\n",
      "ati [281, 0, 677, 7546, 528]\n",
      "mb [282, 1534, 14576, 20610, 7851]\n",
      " J [283, 264, 312, 286, 282]\n",
      "uh [284, 344, 1991, 335, 383]\n",
      " me [285, 1288, 21145, 2722, 1169]\n",
      "nt [286, 126955, 559, 0, 23704]\n",
      "vi [287, 245, 4612, 362, 728]\n",
      "ko [288, 201, 5122, 787, 395]\n",
      "ää [289, 208, 74077, 977, 270]\n",
      " tu [290, 224, 34561, 6797, 1082]\n",
      "ha [291, 315, 1270, 743, 851]\n",
      "sioon [292, 58357, 0, 0, 1005]\n",
      "ndi [293, 0, 0, 0, 0]\n",
      " mis [294, 38374, 13520, 3721, 945]\n",
      " si [295, 228, 8668, 3548, 401]\n",
      "õp [296, 0, 0, 0, 6366]\n",
      "jal [297, 2141, 5123, 4011, 1855]\n",
      "atud [298, 0, 0, 0, 2113]\n",
      " ki [299, 504, 253, 11453, 337]\n",
      " pe [300, 3128, 40618, 12947, 1024]\n",
      "hi [301, 901, 2664, 2865, 2086]\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(token_to_ranks):\n",
    "    if i > 300:\n",
    "        break\n",
    "    print(token, token_to_ranks[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_token(token):\n",
    "    return token.replace('\\t', '\\\\t').replace('\\r', '\\\\r').replace('\\n', '\\\\n')\n",
    "\n",
    "with open(\"../tokenizers/token_ranks\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        escaped_token = escape_token(token)\n",
    "        f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_uniq.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-1] if rank != 0) == 1:\n",
    "            escaped_token = escape_token(token)\n",
    "            f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_not_uniq.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-1] if rank != 0) > 1:\n",
    "            escaped_token = escape_token(token)\n",
    "            f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_all.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-1] if rank != 0) == 4:\n",
    "            escaped_token = escape_token(token)\n",
    "            f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_all_no_se.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-2] if rank != 0) == 3:\n",
    "            escaped_token = escape_token(token)\n",
    "            f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_not_uniq.n3.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-1] if rank != 0) > 1:\n",
    "            escaped_token = escape_token(token)\n",
    "            if len(escaped_token) > 3 and not \"<\" in escaped_token:\n",
    "                f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../tokenizers/token_ranks_all.n3.tsv\", \"w\") as f:\n",
    "    header = \"\\t\".join(vocabs.keys())\n",
    "    f.write(header + \"\\n\")\n",
    "    for token, ranks in token_to_ranks.items():\n",
    "        if sum(1 for rank in ranks[:-1] if rank != 0) == 4:\n",
    "            escaped_token = escape_token(token)\n",
    "            if len(escaped_token) > 3 and not \"<\" in escaped_token:\n",
    "                escaped_token = escaped_token.replace(\" \", \"_\")\n",
    "                f.write(escaped_token + \"\\t\" + \"\\t\".join(map(str, ranks)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
